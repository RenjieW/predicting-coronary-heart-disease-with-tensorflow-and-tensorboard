{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Exploring Data\n",
    "\n",
    "There are 10 variables:\n",
    "\n",
    "* sbp: Systolic blood pressure\n",
    "* tobacco: Cumulative tobacco consumption, in kg\n",
    "* ldl: Low-density lipoprotein cholesterol\n",
    "* adiposity: Adipose tissue concentration\n",
    "* famhist: Family history of heart disease (1=Present, 0=Absent)\n",
    "* typea: Score on test designed to measure type-A behavior\n",
    "* obesity: Obesity\n",
    "* alcohol: Current consumption of alcohol\n",
    "* age: Age of subject\n",
    "* chd: Coronary heart disease at baseline; 1=Yes 0=No\n",
    "\n",
    "\n",
    "Each following row contains the information of one patient. There are 462 samples in total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>Present</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>Absent</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>Present</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>Present</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>Present</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
       "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
       "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
       "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
       "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
       "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/heart.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 462 entries, 0 to 461\n",
      "Data columns (total 10 columns):\n",
      "sbp          462 non-null int64\n",
      "tobacco      462 non-null float64\n",
      "ldl          462 non-null float64\n",
      "adiposity    462 non-null float64\n",
      "famhist      462 non-null object\n",
      "typea        462 non-null int64\n",
      "obesity      462 non-null float64\n",
      "alcohol      462 non-null float64\n",
      "age          462 non-null int64\n",
      "chd          462 non-null int64\n",
      "dtypes: float64(5), int64(4), object(1)\n",
      "memory usage: 36.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "      <th>famhist_Absent</th>\n",
       "      <th>famhist_Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>Present</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>Absent</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>Present</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>Present</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>Present</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd  \\\n",
       "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1   \n",
       "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1   \n",
       "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0   \n",
       "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1   \n",
       "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1   \n",
       "\n",
       "   famhist_Absent  famhist_Present  \n",
       "0               0                1  \n",
       "1               1                0  \n",
       "2               0                1  \n",
       "3               0                1  \n",
       "4               0                1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(data['famhist'],prefix='famhist', drop_first=False)\n",
    "data = pd.concat([data,dummies], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "      <th>famhist_Absent</th>\n",
       "      <th>famhist_Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity  typea  obesity  alcohol  age  chd  \\\n",
       "0  160    12.00  5.73      23.11     49    25.30    97.20   52    1   \n",
       "1  144     0.01  4.41      28.61     55    28.87     2.06   63    1   \n",
       "2  118     0.08  3.48      32.28     52    29.14     3.81   46    0   \n",
       "3  170     7.50  6.41      38.03     51    31.99    24.26   58    1   \n",
       "4  134    13.60  3.50      27.78     60    25.99    57.34   49    1   \n",
       "\n",
       "   famhist_Absent  famhist_Present  \n",
       "0               0                1  \n",
       "1               1                0  \n",
       "2               0                1  \n",
       "3               0                1  \n",
       "4               0                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['famhist'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(data)\n",
    "# scaled_data[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sbp   tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
      "0  0.270642  0.384615  0.309850   0.385267  0.461538  0.227565  0.660371   \n",
      "1  0.197248  0.000321  0.223744   0.514709  0.538462  0.304208  0.013996   \n",
      "2  0.077982  0.002564  0.163079   0.601083  0.500000  0.310004  0.025885   \n",
      "3  0.316514  0.240385  0.354207   0.736409  0.487179  0.371189  0.164821   \n",
      "4  0.151376  0.435897  0.164384   0.495175  0.602564  0.242379  0.389565   \n",
      "\n",
      "        age  chd  famhist_Absent  famhist_Present  \n",
      "0  0.578125    1               0                1  \n",
      "1  0.750000    1               1                0  \n",
      "2  0.484375    0               0                1  \n",
      "3  0.671875    1               0                1  \n",
      "4  0.531250    1               0                1  \n",
      "(462,)\n"
     ]
    }
   ],
   "source": [
    "inputs=['sbp','tobacco','ldl','adiposity','typea','obesity','alcohol','age']\n",
    "\n",
    "labels = data['chd']\n",
    "# min-max scaling\n",
    "for each in inputs:\n",
    "    data[each] = ( data[each] - data[each].min() ) / data[each].max()\n",
    "    \n",
    "print(data.head())\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>famhist_Absent</th>\n",
       "      <th>famhist_Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.309850</td>\n",
       "      <td>0.385267</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.227565</td>\n",
       "      <td>0.660371</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197248</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.223744</td>\n",
       "      <td>0.514709</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.304208</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077982</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.163079</td>\n",
       "      <td>0.601083</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.310004</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.316514</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.354207</td>\n",
       "      <td>0.736409</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.371189</td>\n",
       "      <td>0.164821</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.151376</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.495175</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.242379</td>\n",
       "      <td>0.389565</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sbp   tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
       "0  0.270642  0.384615  0.309850   0.385267  0.461538  0.227565  0.660371   \n",
       "1  0.197248  0.000321  0.223744   0.514709  0.538462  0.304208  0.013996   \n",
       "2  0.077982  0.002564  0.163079   0.601083  0.500000  0.310004  0.025885   \n",
       "3  0.316514  0.240385  0.354207   0.736409  0.487179  0.371189  0.164821   \n",
       "4  0.151376  0.435897  0.164384   0.495175  0.602564  0.242379  0.389565   \n",
       "\n",
       "        age  famhist_Absent  famhist_Present  \n",
       "0  0.578125               0                1  \n",
       "1  0.750000               1                0  \n",
       "2  0.484375               0                1  \n",
       "3  0.671875               0                1  \n",
       "4  0.531250               0                1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.drop(['chd'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labels.head()\n",
    "# binarizer = LabelBinarizer()\n",
    "# # labels = binarizer.fit(lables)\n",
    "\n",
    "# a=tf.one_hot([1,0,1], 2)\n",
    "# with tf.Session as sess:\n",
    "#     sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 462\n"
     ]
    }
   ],
   "source": [
    "features, labels = np.array(features), np.array(labels)\n",
    "print(len(features), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fraction of examples to keep for training\n",
    "split_frac = 0.8\n",
    "n_records = len(features)\n",
    "split_idx = int(split_frac*n_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.70642202e-01,   3.84615385e-01,   3.09849967e-01,\n",
       "          3.85267122e-01,   4.61538462e-01,   2.27565479e-01,\n",
       "          6.60370949e-01,   5.78125000e-01,   0.00000000e+00,\n",
       "          1.00000000e+00],\n",
       "       [  1.97247706e-01,   3.20512821e-04,   2.23744292e-01,\n",
       "          5.14709343e-01,   5.38461538e-01,   3.04207815e-01,\n",
       "          1.39955160e-02,   7.50000000e-01,   1.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  7.79816514e-02,   2.56410256e-03,   1.63078930e-01,\n",
       "          6.01082608e-01,   5.00000000e-01,   3.10004294e-01,\n",
       "          2.58849107e-02,   4.84375000e-01,   0.00000000e+00,\n",
       "          1.00000000e+00],\n",
       "       [  3.16513761e-01,   2.40384615e-01,   3.54207436e-01,\n",
       "          7.36408567e-01,   4.87179487e-01,   3.71189352e-01,\n",
       "          1.64820980e-01,   6.71875000e-01,   0.00000000e+00,\n",
       "          1.00000000e+00],\n",
       "       [  1.51376147e-01,   4.35897436e-01,   1.64383562e-01,\n",
       "          4.95175335e-01,   6.02564103e-01,   2.42378703e-01,\n",
       "          3.89564508e-01,   5.31250000e-01,   0.00000000e+00,\n",
       "          1.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = features[:split_idx], labels[:split_idx]\n",
    "test_X, test_Y = features[split_idx:], labels[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_labels= 2\n",
    "n_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_epochs= 200\n",
    "n_hidden1 = 5\n",
    "# batch_size = 128\n",
    "# display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32,[None, 10], name ='inputs' )\n",
    "    labels = tf.placeholder(tf.int32, [None,], name='output')\n",
    "    labels_one_hot = tf.one_hot(labels, 2)\n",
    "    \n",
    "    weights = {\n",
    "        'hidden_layer': tf.Variable(tf.truncated_normal([n_features,n_hidden1], stddev=0.1)),\n",
    "        'output':tf.Variable(tf.truncated_normal([n_hidden1, n_labels], stddev=0.1))\n",
    "    }\n",
    "    \n",
    "    bias = {\n",
    "        'hidden_layer':tf.Variable(tf.zeros([n_hidden1])),\n",
    "        'output':tf.Variable(tf.zeros(n_labels))\n",
    "    }\n",
    "    \n",
    "    hidden_layer = tf.nn.bias_add(tf.matmul(inputs,weights['hidden_layer']), bias['hidden_layer'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    \n",
    "    logits = tf.nn.bias_add(tf.matmul(hidden_layer, weights['output']), bias['output'])\n",
    "    \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_one_hot)\n",
    "    cost = tf.reduce_mean(entropy)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        #tensorboard\n",
    "        file_writer = tf.summary.FileWriter('./logs/1', sess.graph)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={inputs:train_X, labels:train_Y})\n",
    "           \n",
    "            print(\"Epoch: {0} ; training loss: {1}\".format(epoch, loss))\n",
    "            \n",
    "        print('training finished')\n",
    "        \n",
    "         # testing the model on test data\n",
    "            \n",
    "#         test_loss,logits = sess.run([loss,logits],feed_dict={inputs:test_X,labels:test_Y})\n",
    "        \n",
    "#         predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "#         correct_preds = tf.equal(tf.argmax(predictions, 1), tf.argmax(tf.one_hot(test_Y), 1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32)) \n",
    "        \n",
    "#         print('model accuracy : {}'.format(accuracy))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_one_hot, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Accuracy:\", accuracy.eval({inputs: test_X, labels: test_Y}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ; training loss: 0.6902515292167664\n",
      "Epoch: 1 ; training loss: 0.6863057017326355\n",
      "Epoch: 2 ; training loss: 0.6827540993690491\n",
      "Epoch: 3 ; training loss: 0.6795487999916077\n",
      "Epoch: 4 ; training loss: 0.6766538619995117\n",
      "Epoch: 5 ; training loss: 0.674036979675293\n",
      "Epoch: 6 ; training loss: 0.6716666221618652\n",
      "Epoch: 7 ; training loss: 0.6695170402526855\n",
      "Epoch: 8 ; training loss: 0.6675642728805542\n",
      "Epoch: 9 ; training loss: 0.6657883524894714\n",
      "Epoch: 10 ; training loss: 0.6641700863838196\n",
      "Epoch: 11 ; training loss: 0.6626936197280884\n",
      "Epoch: 12 ; training loss: 0.6613407135009766\n",
      "Epoch: 13 ; training loss: 0.6601017117500305\n",
      "Epoch: 14 ; training loss: 0.6589651107788086\n",
      "Epoch: 15 ; training loss: 0.6579157710075378\n",
      "Epoch: 16 ; training loss: 0.656948447227478\n",
      "Epoch: 17 ; training loss: 0.6560526490211487\n",
      "Epoch: 18 ; training loss: 0.6552185416221619\n",
      "Epoch: 19 ; training loss: 0.6544349789619446\n",
      "Epoch: 20 ; training loss: 0.6536989212036133\n",
      "Epoch: 21 ; training loss: 0.6530072689056396\n",
      "Epoch: 22 ; training loss: 0.6523544788360596\n",
      "Epoch: 23 ; training loss: 0.651737630367279\n",
      "Epoch: 24 ; training loss: 0.6511496305465698\n",
      "Epoch: 25 ; training loss: 0.6505876183509827\n",
      "Epoch: 26 ; training loss: 0.65004962682724\n",
      "Epoch: 27 ; training loss: 0.6495339870452881\n",
      "Epoch: 28 ; training loss: 0.649034857749939\n",
      "Epoch: 29 ; training loss: 0.6485535502433777\n",
      "Epoch: 30 ; training loss: 0.6480820775032043\n",
      "Epoch: 31 ; training loss: 0.6476221084594727\n",
      "Epoch: 32 ; training loss: 0.6471776366233826\n",
      "Epoch: 33 ; training loss: 0.6467334628105164\n",
      "Epoch: 34 ; training loss: 0.6462987065315247\n",
      "Epoch: 35 ; training loss: 0.6458627581596375\n",
      "Epoch: 36 ; training loss: 0.6454393863677979\n",
      "Epoch: 37 ; training loss: 0.6450384855270386\n",
      "Epoch: 38 ; training loss: 0.6446515321731567\n",
      "Epoch: 39 ; training loss: 0.6442813277244568\n",
      "Epoch: 40 ; training loss: 0.6439134478569031\n",
      "Epoch: 41 ; training loss: 0.6435601115226746\n",
      "Epoch: 42 ; training loss: 0.6432186961174011\n",
      "Epoch: 43 ; training loss: 0.6428778767585754\n",
      "Epoch: 44 ; training loss: 0.6425366997718811\n",
      "Epoch: 45 ; training loss: 0.6422047019004822\n",
      "Epoch: 46 ; training loss: 0.6418771147727966\n",
      "Epoch: 47 ; training loss: 0.6415523290634155\n",
      "Epoch: 48 ; training loss: 0.6412299275398254\n",
      "Epoch: 49 ; training loss: 0.6409093737602234\n",
      "Epoch: 50 ; training loss: 0.6405877470970154\n",
      "Epoch: 51 ; training loss: 0.6402651071548462\n",
      "Epoch: 52 ; training loss: 0.6399425268173218\n",
      "Epoch: 53 ; training loss: 0.6396178603172302\n",
      "Epoch: 54 ; training loss: 0.639290988445282\n",
      "Epoch: 55 ; training loss: 0.6389628052711487\n",
      "Epoch: 56 ; training loss: 0.638629138469696\n",
      "Epoch: 57 ; training loss: 0.6382971405982971\n",
      "Epoch: 58 ; training loss: 0.637965738773346\n",
      "Epoch: 59 ; training loss: 0.6376341581344604\n",
      "Epoch: 60 ; training loss: 0.6373012661933899\n",
      "Epoch: 61 ; training loss: 0.6369699835777283\n",
      "Epoch: 62 ; training loss: 0.6366339921951294\n",
      "Epoch: 63 ; training loss: 0.636293351650238\n",
      "Epoch: 64 ; training loss: 0.6359485387802124\n",
      "Epoch: 65 ; training loss: 0.6356086730957031\n",
      "Epoch: 66 ; training loss: 0.6352678537368774\n",
      "Epoch: 67 ; training loss: 0.6349244713783264\n",
      "Epoch: 68 ; training loss: 0.6345767378807068\n",
      "Epoch: 69 ; training loss: 0.6342288851737976\n",
      "Epoch: 70 ; training loss: 0.6338837146759033\n",
      "Epoch: 71 ; training loss: 0.633536159992218\n",
      "Epoch: 72 ; training loss: 0.6331848502159119\n",
      "Epoch: 73 ; training loss: 0.632830023765564\n",
      "Epoch: 74 ; training loss: 0.6324719786643982\n",
      "Epoch: 75 ; training loss: 0.6321120262145996\n",
      "Epoch: 76 ; training loss: 0.6317476034164429\n",
      "Epoch: 77 ; training loss: 0.6313821077346802\n",
      "Epoch: 78 ; training loss: 0.6310163736343384\n",
      "Epoch: 79 ; training loss: 0.6306504011154175\n",
      "Epoch: 80 ; training loss: 0.6302828192710876\n",
      "Epoch: 81 ; training loss: 0.6299130320549011\n",
      "Epoch: 82 ; training loss: 0.6295408606529236\n",
      "Epoch: 83 ; training loss: 0.629166841506958\n",
      "Epoch: 84 ; training loss: 0.6287907958030701\n",
      "Epoch: 85 ; training loss: 0.628412663936615\n",
      "Epoch: 86 ; training loss: 0.6280326247215271\n",
      "Epoch: 87 ; training loss: 0.627650797367096\n",
      "Epoch: 88 ; training loss: 0.6272668242454529\n",
      "Epoch: 89 ; training loss: 0.626880943775177\n",
      "Epoch: 90 ; training loss: 0.6264931559562683\n",
      "Epoch: 91 ; training loss: 0.6261036396026611\n",
      "Epoch: 92 ; training loss: 0.6257126927375793\n",
      "Epoch: 93 ; training loss: 0.6253201365470886\n",
      "Epoch: 94 ; training loss: 0.6249264478683472\n",
      "Epoch: 95 ; training loss: 0.6245315074920654\n",
      "Epoch: 96 ; training loss: 0.6241350769996643\n",
      "Epoch: 97 ; training loss: 0.6237375140190125\n",
      "Epoch: 98 ; training loss: 0.6233387589454651\n",
      "Epoch: 99 ; training loss: 0.6229392290115356\n",
      "Epoch: 100 ; training loss: 0.6225394606590271\n",
      "Epoch: 101 ; training loss: 0.6221386790275574\n",
      "Epoch: 102 ; training loss: 0.6217371821403503\n",
      "Epoch: 103 ; training loss: 0.6213352084159851\n",
      "Epoch: 104 ; training loss: 0.6209329962730408\n",
      "Epoch: 105 ; training loss: 0.6205304861068726\n",
      "Epoch: 106 ; training loss: 0.6201278567314148\n",
      "Epoch: 107 ; training loss: 0.6197252869606018\n",
      "Epoch: 108 ; training loss: 0.6193228363990784\n",
      "Epoch: 109 ; training loss: 0.618920624256134\n",
      "Epoch: 110 ; training loss: 0.6185188889503479\n",
      "Epoch: 111 ; training loss: 0.6181176900863647\n",
      "Epoch: 112 ; training loss: 0.617717444896698\n",
      "Epoch: 113 ; training loss: 0.617317795753479\n",
      "Epoch: 114 ; training loss: 0.6169190406799316\n",
      "Epoch: 115 ; training loss: 0.6165210008621216\n",
      "Epoch: 116 ; training loss: 0.6161240339279175\n",
      "Epoch: 117 ; training loss: 0.6157282590866089\n",
      "Epoch: 118 ; training loss: 0.6153336763381958\n",
      "Epoch: 119 ; training loss: 0.6149404644966125\n",
      "Epoch: 120 ; training loss: 0.6145483255386353\n",
      "Epoch: 121 ; training loss: 0.6141574382781982\n",
      "Epoch: 122 ; training loss: 0.6137681007385254\n",
      "Epoch: 123 ; training loss: 0.6133801341056824\n",
      "Epoch: 124 ; training loss: 0.6129940152168274\n",
      "Epoch: 125 ; training loss: 0.6126090884208679\n",
      "Epoch: 126 ; training loss: 0.6122267842292786\n",
      "Epoch: 127 ; training loss: 0.6118471622467041\n",
      "Epoch: 128 ; training loss: 0.6114694476127625\n",
      "Epoch: 129 ; training loss: 0.611094057559967\n",
      "Epoch: 130 ; training loss: 0.6107209324836731\n",
      "Epoch: 131 ; training loss: 0.6103506684303284\n",
      "Epoch: 132 ; training loss: 0.6099833846092224\n",
      "Epoch: 133 ; training loss: 0.6096185445785522\n",
      "Epoch: 134 ; training loss: 0.609257161617279\n",
      "Epoch: 135 ; training loss: 0.6088986396789551\n",
      "Epoch: 136 ; training loss: 0.6085428595542908\n",
      "Epoch: 137 ; training loss: 0.6081894636154175\n",
      "Epoch: 138 ; training loss: 0.6078392267227173\n",
      "Epoch: 139 ; training loss: 0.6074917316436768\n",
      "Epoch: 140 ; training loss: 0.607147216796875\n",
      "Epoch: 141 ; training loss: 0.6068050861358643\n",
      "Epoch: 142 ; training loss: 0.6064662337303162\n",
      "Epoch: 143 ; training loss: 0.6061301231384277\n",
      "Epoch: 144 ; training loss: 0.6057972311973572\n",
      "Epoch: 145 ; training loss: 0.6054667830467224\n",
      "Epoch: 146 ; training loss: 0.6051388382911682\n",
      "Epoch: 147 ; training loss: 0.6048136949539185\n",
      "Epoch: 148 ; training loss: 0.6044914126396179\n",
      "Epoch: 149 ; training loss: 0.6041717529296875\n",
      "Epoch: 150 ; training loss: 0.6038554310798645\n",
      "Epoch: 151 ; training loss: 0.6035422086715698\n",
      "Epoch: 152 ; training loss: 0.6032301187515259\n",
      "Epoch: 153 ; training loss: 0.6029215455055237\n",
      "Epoch: 154 ; training loss: 0.6026151776313782\n",
      "Epoch: 155 ; training loss: 0.6023118495941162\n",
      "Epoch: 156 ; training loss: 0.6020111441612244\n",
      "Epoch: 157 ; training loss: 0.6017125844955444\n",
      "Epoch: 158 ; training loss: 0.6014155745506287\n",
      "Epoch: 159 ; training loss: 0.6011207103729248\n",
      "Epoch: 160 ; training loss: 0.6008293032646179\n",
      "Epoch: 161 ; training loss: 0.6005410552024841\n",
      "Epoch: 162 ; training loss: 0.6002559661865234\n",
      "Epoch: 163 ; training loss: 0.5999738574028015\n",
      "Epoch: 164 ; training loss: 0.5996946692466736\n",
      "Epoch: 165 ; training loss: 0.5994186401367188\n",
      "Epoch: 166 ; training loss: 0.5991451144218445\n",
      "Epoch: 167 ; training loss: 0.5988739132881165\n",
      "Epoch: 168 ; training loss: 0.5986046195030212\n",
      "Epoch: 169 ; training loss: 0.5983371138572693\n",
      "Epoch: 170 ; training loss: 0.598071277141571\n",
      "Epoch: 171 ; training loss: 0.5978068709373474\n",
      "Epoch: 172 ; training loss: 0.5975423455238342\n",
      "Epoch: 173 ; training loss: 0.5972793102264404\n",
      "Epoch: 174 ; training loss: 0.5970181226730347\n",
      "Epoch: 175 ; training loss: 0.5967578887939453\n",
      "Epoch: 176 ; training loss: 0.5964991450309753\n",
      "Epoch: 177 ; training loss: 0.5962422490119934\n",
      "Epoch: 178 ; training loss: 0.5959873199462891\n",
      "Epoch: 179 ; training loss: 0.5957331657409668\n",
      "Epoch: 180 ; training loss: 0.5954782366752625\n",
      "Epoch: 181 ; training loss: 0.595221996307373\n",
      "Epoch: 182 ; training loss: 0.5949618816375732\n",
      "Epoch: 183 ; training loss: 0.594707190990448\n",
      "Epoch: 184 ; training loss: 0.5944555997848511\n",
      "Epoch: 185 ; training loss: 0.5942073464393616\n",
      "Epoch: 186 ; training loss: 0.5939564108848572\n",
      "Epoch: 187 ; training loss: 0.5937072038650513\n",
      "Epoch: 188 ; training loss: 0.59345942735672\n",
      "Epoch: 189 ; training loss: 0.5932132005691528\n",
      "Epoch: 190 ; training loss: 0.5929656028747559\n",
      "Epoch: 191 ; training loss: 0.592716634273529\n",
      "Epoch: 192 ; training loss: 0.5924715995788574\n",
      "Epoch: 193 ; training loss: 0.5922272801399231\n",
      "Epoch: 194 ; training loss: 0.5919803977012634\n",
      "Epoch: 195 ; training loss: 0.5917284488677979\n",
      "Epoch: 196 ; training loss: 0.5914823412895203\n",
      "Epoch: 197 ; training loss: 0.5912368297576904\n",
      "Epoch: 198 ; training loss: 0.5909940600395203\n",
      "Epoch: 199 ; training loss: 0.5907575488090515\n",
      "training finished\n",
      "Accuracy: 0.741935\n"
     ]
    }
   ],
   "source": [
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scoping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_2():\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32,[None, 10], name ='inputs' )\n",
    "        \n",
    "    with tf.name_scope('target_labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None,], name='output')\n",
    "        labels_one_hot = tf.one_hot(labels, 2)\n",
    "    \n",
    "    with tf.name_scope('weights'):\n",
    "        weights = {\n",
    "            'hidden_layer': tf.Variable(tf.truncated_normal([n_features,n_hidden1], stddev=0.1), name='hidden_weights'),\n",
    "            'output':tf.Variable(tf.truncated_normal([n_hidden1, n_labels], stddev=0.1), name='output_weights')\n",
    "        }\n",
    "    \n",
    "    with tf.name_scope('biases'):\n",
    "    \n",
    "        bias = {\n",
    "            'hidden_layer':tf.Variable(tf.zeros([n_hidden1]), name='hidden_biases'),\n",
    "            'output':tf.Variable(tf.zeros(n_labels), name='output_biases')\n",
    "        }\n",
    "        \n",
    "    with tf.name_scope('hidden_layers'):\n",
    "\n",
    "        hidden_layer = tf.nn.bias_add(tf.matmul(inputs,weights['hidden_layer']), bias['hidden_layer'])\n",
    "        hidden_layer = tf.nn.relu(hidden_layer, name='hidden_layer_output')\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "\n",
    "        logits = tf.nn.bias_add(tf.matmul(hidden_layer, weights['output']), bias['output'], name='predictions')\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_one_hot, name='cross_entropy')\n",
    "        cost = tf.reduce_mean(entropy, name='cost')\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        #tensorboard\n",
    "        file_writer = tf.summary.FileWriter('./logs/2', sess.graph)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={inputs:train_X, labels:train_Y})\n",
    "           \n",
    "            print(\"Epoch: {0} ; training loss: {1}\".format(epoch, loss))\n",
    "            \n",
    "        print('training finished')\n",
    "        \n",
    "         # testing the model on test data\n",
    "            \n",
    "#         test_loss,logits = sess.run([loss,logits],feed_dict={inputs:test_X,labels:test_Y})\n",
    "        \n",
    "#         predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "#         correct_preds = tf.equal(tf.argmax(predictions, 1), tf.argmax(tf.one_hot(test_Y), 1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32)) \n",
    "        \n",
    "#         print('model accuracy : {}'.format(accuracy))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_one_hot, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Accuracy:\", accuracy.eval({inputs: test_X, labels: test_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ; training loss: 0.6895103454589844\n",
      "Epoch: 1 ; training loss: 0.6854351758956909\n",
      "Epoch: 2 ; training loss: 0.6817319989204407\n",
      "Epoch: 3 ; training loss: 0.678347647190094\n",
      "Epoch: 4 ; training loss: 0.6752737164497375\n",
      "Epoch: 5 ; training loss: 0.6724792718887329\n",
      "Epoch: 6 ; training loss: 0.669940710067749\n",
      "Epoch: 7 ; training loss: 0.6676146388053894\n",
      "Epoch: 8 ; training loss: 0.6654449105262756\n",
      "Epoch: 9 ; training loss: 0.663406252861023\n",
      "Epoch: 10 ; training loss: 0.6615357995033264\n",
      "Epoch: 11 ; training loss: 0.659821093082428\n",
      "Epoch: 12 ; training loss: 0.6582509875297546\n",
      "Epoch: 13 ; training loss: 0.6568374633789062\n",
      "Epoch: 14 ; training loss: 0.6555294990539551\n",
      "Epoch: 15 ; training loss: 0.6543349623680115\n",
      "Epoch: 16 ; training loss: 0.6532660722732544\n",
      "Epoch: 17 ; training loss: 0.6522902250289917\n",
      "Epoch: 18 ; training loss: 0.6513990163803101\n",
      "Epoch: 19 ; training loss: 0.6505838632583618\n",
      "Epoch: 20 ; training loss: 0.6498392820358276\n",
      "Epoch: 21 ; training loss: 0.6491523385047913\n",
      "Epoch: 22 ; training loss: 0.6485145688056946\n",
      "Epoch: 23 ; training loss: 0.6479223966598511\n",
      "Epoch: 24 ; training loss: 0.6473710536956787\n",
      "Epoch: 25 ; training loss: 0.6468544602394104\n",
      "Epoch: 26 ; training loss: 0.6463664770126343\n",
      "Epoch: 27 ; training loss: 0.6459084153175354\n",
      "Epoch: 28 ; training loss: 0.6454772353172302\n",
      "Epoch: 29 ; training loss: 0.6450672745704651\n",
      "Epoch: 30 ; training loss: 0.6446740031242371\n",
      "Epoch: 31 ; training loss: 0.6442984938621521\n",
      "Epoch: 32 ; training loss: 0.6439350247383118\n",
      "Epoch: 33 ; training loss: 0.6435851454734802\n",
      "Epoch: 34 ; training loss: 0.6432464718818665\n",
      "Epoch: 35 ; training loss: 0.6429188251495361\n",
      "Epoch: 36 ; training loss: 0.6426005363464355\n",
      "Epoch: 37 ; training loss: 0.6422879099845886\n",
      "Epoch: 38 ; training loss: 0.6419809460639954\n",
      "Epoch: 39 ; training loss: 0.641681432723999\n",
      "Epoch: 40 ; training loss: 0.641385018825531\n",
      "Epoch: 41 ; training loss: 0.6410923004150391\n",
      "Epoch: 42 ; training loss: 0.6408048868179321\n",
      "Epoch: 43 ; training loss: 0.6405216455459595\n",
      "Epoch: 44 ; training loss: 0.6402419805526733\n",
      "Epoch: 45 ; training loss: 0.6399627327919006\n",
      "Epoch: 46 ; training loss: 0.6396844387054443\n",
      "Epoch: 47 ; training loss: 0.6394076347351074\n",
      "Epoch: 48 ; training loss: 0.639130175113678\n",
      "Epoch: 49 ; training loss: 0.6388501524925232\n",
      "Epoch: 50 ; training loss: 0.6385700702667236\n",
      "Epoch: 51 ; training loss: 0.6382904648780823\n",
      "Epoch: 52 ; training loss: 0.6380113959312439\n",
      "Epoch: 53 ; training loss: 0.6377334594726562\n",
      "Epoch: 54 ; training loss: 0.6374534368515015\n",
      "Epoch: 55 ; training loss: 0.6371715664863586\n",
      "Epoch: 56 ; training loss: 0.6368882656097412\n",
      "Epoch: 57 ; training loss: 0.6366057991981506\n",
      "Epoch: 58 ; training loss: 0.636325478553772\n",
      "Epoch: 59 ; training loss: 0.6360448598861694\n",
      "Epoch: 60 ; training loss: 0.6357631683349609\n",
      "Epoch: 61 ; training loss: 0.6354827284812927\n",
      "Epoch: 62 ; training loss: 0.6352022290229797\n",
      "Epoch: 63 ; training loss: 0.6349217295646667\n",
      "Epoch: 64 ; training loss: 0.6346395015716553\n",
      "Epoch: 65 ; training loss: 0.6343546509742737\n",
      "Epoch: 66 ; training loss: 0.634070098400116\n",
      "Epoch: 67 ; training loss: 0.6337854862213135\n",
      "Epoch: 68 ; training loss: 0.6335010528564453\n",
      "Epoch: 69 ; training loss: 0.6332160234451294\n",
      "Epoch: 70 ; training loss: 0.6329295635223389\n",
      "Epoch: 71 ; training loss: 0.6326414346694946\n",
      "Epoch: 72 ; training loss: 0.6323537230491638\n",
      "Epoch: 73 ; training loss: 0.6320644617080688\n",
      "Epoch: 74 ; training loss: 0.6317716836929321\n",
      "Epoch: 75 ; training loss: 0.6314782500267029\n",
      "Epoch: 76 ; training loss: 0.6311851143836975\n",
      "Epoch: 77 ; training loss: 0.6308915615081787\n",
      "Epoch: 78 ; training loss: 0.6305972933769226\n",
      "Epoch: 79 ; training loss: 0.6303020119667053\n",
      "Epoch: 80 ; training loss: 0.6300061345100403\n",
      "Epoch: 81 ; training loss: 0.6297093629837036\n",
      "Epoch: 82 ; training loss: 0.6294117569923401\n",
      "Epoch: 83 ; training loss: 0.6291123032569885\n",
      "Epoch: 84 ; training loss: 0.628811776638031\n",
      "Epoch: 85 ; training loss: 0.6285102963447571\n",
      "Epoch: 86 ; training loss: 0.6282085180282593\n",
      "Epoch: 87 ; training loss: 0.627905547618866\n",
      "Epoch: 88 ; training loss: 0.6276010870933533\n",
      "Epoch: 89 ; training loss: 0.6272957921028137\n",
      "Epoch: 90 ; training loss: 0.6269893646240234\n",
      "Epoch: 91 ; training loss: 0.6266814470291138\n",
      "Epoch: 92 ; training loss: 0.6263726949691772\n",
      "Epoch: 93 ; training loss: 0.6260634064674377\n",
      "Epoch: 94 ; training loss: 0.6257529854774475\n",
      "Epoch: 95 ; training loss: 0.6254417300224304\n",
      "Epoch: 96 ; training loss: 0.6251307725906372\n",
      "Epoch: 97 ; training loss: 0.6248191595077515\n",
      "Epoch: 98 ; training loss: 0.6245083212852478\n",
      "Epoch: 99 ; training loss: 0.624196469783783\n",
      "Epoch: 100 ; training loss: 0.6238849759101868\n",
      "Epoch: 101 ; training loss: 0.6235727667808533\n",
      "Epoch: 102 ; training loss: 0.6232609152793884\n",
      "Epoch: 103 ; training loss: 0.6229487061500549\n",
      "Epoch: 104 ; training loss: 0.6226360201835632\n",
      "Epoch: 105 ; training loss: 0.6223224997520447\n",
      "Epoch: 106 ; training loss: 0.6220085620880127\n",
      "Epoch: 107 ; training loss: 0.6216937899589539\n",
      "Epoch: 108 ; training loss: 0.621380090713501\n",
      "Epoch: 109 ; training loss: 0.6210652589797974\n",
      "Epoch: 110 ; training loss: 0.6207504272460938\n",
      "Epoch: 111 ; training loss: 0.6204346418380737\n",
      "Epoch: 112 ; training loss: 0.6201177835464478\n",
      "Epoch: 113 ; training loss: 0.6198003888130188\n",
      "Epoch: 114 ; training loss: 0.6194827556610107\n",
      "Epoch: 115 ; training loss: 0.6191647052764893\n",
      "Epoch: 116 ; training loss: 0.6188454031944275\n",
      "Epoch: 117 ; training loss: 0.6185265779495239\n",
      "Epoch: 118 ; training loss: 0.618208110332489\n",
      "Epoch: 119 ; training loss: 0.6178895831108093\n",
      "Epoch: 120 ; training loss: 0.6175723075866699\n",
      "Epoch: 121 ; training loss: 0.6172574162483215\n",
      "Epoch: 122 ; training loss: 0.6169441342353821\n",
      "Epoch: 123 ; training loss: 0.6166307926177979\n",
      "Epoch: 124 ; training loss: 0.6163175106048584\n",
      "Epoch: 125 ; training loss: 0.6160061359405518\n",
      "Epoch: 126 ; training loss: 0.6156944632530212\n",
      "Epoch: 127 ; training loss: 0.6153842806816101\n",
      "Epoch: 128 ; training loss: 0.6150729060173035\n",
      "Epoch: 129 ; training loss: 0.6147620677947998\n",
      "Epoch: 130 ; training loss: 0.614452064037323\n",
      "Epoch: 131 ; training loss: 0.6141427159309387\n",
      "Epoch: 132 ; training loss: 0.6138327717781067\n",
      "Epoch: 133 ; training loss: 0.6135214567184448\n",
      "Epoch: 134 ; training loss: 0.6132111549377441\n",
      "Epoch: 135 ; training loss: 0.6129025220870972\n",
      "Epoch: 136 ; training loss: 0.6125965118408203\n",
      "Epoch: 137 ; training loss: 0.6122921705245972\n",
      "Epoch: 138 ; training loss: 0.6119887828826904\n",
      "Epoch: 139 ; training loss: 0.6116884350776672\n",
      "Epoch: 140 ; training loss: 0.6113880276679993\n",
      "Epoch: 141 ; training loss: 0.6110872030258179\n",
      "Epoch: 142 ; training loss: 0.6107868552207947\n",
      "Epoch: 143 ; training loss: 0.6104885935783386\n",
      "Epoch: 144 ; training loss: 0.6101922988891602\n",
      "Epoch: 145 ; training loss: 0.6098966002464294\n",
      "Epoch: 146 ; training loss: 0.6096020936965942\n",
      "Epoch: 147 ; training loss: 0.609309196472168\n",
      "Epoch: 148 ; training loss: 0.6090174317359924\n",
      "Epoch: 149 ; training loss: 0.6087260842323303\n",
      "Epoch: 150 ; training loss: 0.6084344387054443\n",
      "Epoch: 151 ; training loss: 0.6081429719924927\n",
      "Epoch: 152 ; training loss: 0.6078513264656067\n",
      "Epoch: 153 ; training loss: 0.607560396194458\n",
      "Epoch: 154 ; training loss: 0.6072697639465332\n",
      "Epoch: 155 ; training loss: 0.6069814562797546\n",
      "Epoch: 156 ; training loss: 0.6066943407058716\n",
      "Epoch: 157 ; training loss: 0.6064078211784363\n",
      "Epoch: 158 ; training loss: 0.6061234474182129\n",
      "Epoch: 159 ; training loss: 0.6058404445648193\n",
      "Epoch: 160 ; training loss: 0.6055591106414795\n",
      "Epoch: 161 ; training loss: 0.6052784323692322\n",
      "Epoch: 162 ; training loss: 0.6049975752830505\n",
      "Epoch: 163 ; training loss: 0.6047170758247375\n",
      "Epoch: 164 ; training loss: 0.6044359803199768\n",
      "Epoch: 165 ; training loss: 0.6041536331176758\n",
      "Epoch: 166 ; training loss: 0.6038716435432434\n",
      "Epoch: 167 ; training loss: 0.6035906076431274\n",
      "Epoch: 168 ; training loss: 0.6033108234405518\n",
      "Epoch: 169 ; training loss: 0.6030312180519104\n",
      "Epoch: 170 ; training loss: 0.6027509570121765\n",
      "Epoch: 171 ; training loss: 0.6024714112281799\n",
      "Epoch: 172 ; training loss: 0.6021924018859863\n",
      "Epoch: 173 ; training loss: 0.6019151210784912\n",
      "Epoch: 174 ; training loss: 0.6016377806663513\n",
      "Epoch: 175 ; training loss: 0.6013604998588562\n",
      "Epoch: 176 ; training loss: 0.6010847091674805\n",
      "Epoch: 177 ; training loss: 0.6008102893829346\n",
      "Epoch: 178 ; training loss: 0.600536048412323\n",
      "Epoch: 179 ; training loss: 0.6002628207206726\n",
      "Epoch: 180 ; training loss: 0.5999910235404968\n",
      "Epoch: 181 ; training loss: 0.5997209548950195\n",
      "Epoch: 182 ; training loss: 0.599452555179596\n",
      "Epoch: 183 ; training loss: 0.599185585975647\n",
      "Epoch: 184 ; training loss: 0.5989183187484741\n",
      "Epoch: 185 ; training loss: 0.5986518263816833\n",
      "Epoch: 186 ; training loss: 0.5983865261077881\n",
      "Epoch: 187 ; training loss: 0.5981228947639465\n",
      "Epoch: 188 ; training loss: 0.597861111164093\n",
      "Epoch: 189 ; training loss: 0.5976006984710693\n",
      "Epoch: 190 ; training loss: 0.5973416566848755\n",
      "Epoch: 191 ; training loss: 0.5970838069915771\n",
      "Epoch: 192 ; training loss: 0.5968272089958191\n",
      "Epoch: 193 ; training loss: 0.5965713262557983\n",
      "Epoch: 194 ; training loss: 0.596316933631897\n",
      "Epoch: 195 ; training loss: 0.5960631966590881\n",
      "Epoch: 196 ; training loss: 0.5958102941513062\n",
      "Epoch: 197 ; training loss: 0.5955575108528137\n",
      "Epoch: 198 ; training loss: 0.5953056812286377\n",
      "Epoch: 199 ; training loss: 0.5950539708137512\n",
      "training finished\n",
      "Accuracy: 0.774194\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising weights and distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_3():\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32,[None, 10], name ='inputs' )\n",
    "        \n",
    "    with tf.name_scope('target_labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None,], name='output')\n",
    "        labels_one_hot = tf.one_hot(labels, 2)\n",
    "    \n",
    "    with tf.name_scope('weights'):\n",
    "        weights = {\n",
    "            'hidden_layer': tf.Variable(tf.truncated_normal([n_features,n_hidden1], stddev=0.1), name='hidden_weights'),\n",
    "            'output':tf.Variable(tf.truncated_normal([n_hidden1, n_labels], stddev=0.1), name='output_weights')\n",
    "        }\n",
    "        \n",
    "        tf.summary.histogram('hidden_weights', weights['hidden_layer'])\n",
    "        tf.summary.histogram('output_weights', weights['output'])\n",
    "    \n",
    "    with tf.name_scope('biases'):\n",
    "    \n",
    "        bias = {\n",
    "            'hidden_layer':tf.Variable(tf.zeros([n_hidden1]), name='hidden_biases'),\n",
    "            'output':tf.Variable(tf.zeros(n_labels), name='output_biases')\n",
    "        }\n",
    "        \n",
    "        tf.summary.histogram('hidden_biases', bias['hidden_layer'])\n",
    "        tf.summary.histogram('output_biases', bias['output'])\n",
    "        \n",
    "    with tf.name_scope('hidden_layers'):\n",
    "\n",
    "        hidden_layer = tf.nn.bias_add(tf.matmul(inputs,weights['hidden_layer']), bias['hidden_layer'])\n",
    "        hidden_layer = tf.nn.relu(hidden_layer, name='hidden_layer_output')\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "\n",
    "        logits = tf.nn.bias_add(tf.matmul(hidden_layer, weights['output']), bias['output'], name='logits')\n",
    "        pred = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', pred)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_one_hot, name='cross_entropy')\n",
    "        cost = tf.reduce_mean(entropy, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "        \n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        #tensorboard\n",
    "        train_writer = tf.summary.FileWriter('./logs/3', sess.graph)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            summary,_, loss = sess.run([merged,optimizer, cost], feed_dict={inputs:train_X, labels:train_Y})\n",
    "           \n",
    "            print(\"Epoch: {0} ; training loss: {1}\".format(epoch, loss))\n",
    "            \n",
    "            train_writer.add_summary(summary, epoch+1)\n",
    "            \n",
    "        print('training finished')\n",
    "        \n",
    "         # testing the model on test data\n",
    "            \n",
    "#         test_loss,logits = sess.run([loss,logits],feed_dict={inputs:test_X,labels:test_Y})\n",
    "        \n",
    "#         predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "#         correct_preds = tf.equal(tf.argmax(predictions, 1), tf.argmax(tf.one_hot(test_Y), 1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32)) \n",
    "        \n",
    "#         print('model accuracy : {}'.format(accuracy))\n",
    "        \n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_one_hot, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Accuracy:\", accuracy.eval({inputs: test_X, labels: test_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ; training loss: 0.6924264430999756\n",
      "Epoch: 1 ; training loss: 0.6878156661987305\n",
      "Epoch: 2 ; training loss: 0.6836753487586975\n",
      "Epoch: 3 ; training loss: 0.6799539923667908\n",
      "Epoch: 4 ; training loss: 0.6766065955162048\n",
      "Epoch: 5 ; training loss: 0.6735934615135193\n",
      "Epoch: 6 ; training loss: 0.67087721824646\n",
      "Epoch: 7 ; training loss: 0.6684249639511108\n",
      "Epoch: 8 ; training loss: 0.6662085652351379\n",
      "Epoch: 9 ; training loss: 0.6642038822174072\n",
      "Epoch: 10 ; training loss: 0.6623884439468384\n",
      "Epoch: 11 ; training loss: 0.6607438921928406\n",
      "Epoch: 12 ; training loss: 0.65925133228302\n",
      "Epoch: 13 ; training loss: 0.6578938364982605\n",
      "Epoch: 14 ; training loss: 0.6566564440727234\n",
      "Epoch: 15 ; training loss: 0.6555258631706238\n",
      "Epoch: 16 ; training loss: 0.6544875502586365\n",
      "Epoch: 17 ; training loss: 0.653531014919281\n",
      "Epoch: 18 ; training loss: 0.6526522636413574\n",
      "Epoch: 19 ; training loss: 0.6518397331237793\n",
      "Epoch: 20 ; training loss: 0.6510838866233826\n",
      "Epoch: 21 ; training loss: 0.6503826975822449\n",
      "Epoch: 22 ; training loss: 0.649729311466217\n",
      "Epoch: 23 ; training loss: 0.649121880531311\n",
      "Epoch: 24 ; training loss: 0.6485524773597717\n",
      "Epoch: 25 ; training loss: 0.6480208039283752\n",
      "Epoch: 26 ; training loss: 0.647520124912262\n",
      "Epoch: 27 ; training loss: 0.6470412015914917\n",
      "Epoch: 28 ; training loss: 0.6465899348258972\n",
      "Epoch: 29 ; training loss: 0.6461585760116577\n",
      "Epoch: 30 ; training loss: 0.6457396745681763\n",
      "Epoch: 31 ; training loss: 0.6453388333320618\n",
      "Epoch: 32 ; training loss: 0.644953191280365\n",
      "Epoch: 33 ; training loss: 0.6445786952972412\n",
      "Epoch: 34 ; training loss: 0.6442122459411621\n",
      "Epoch: 35 ; training loss: 0.6438565254211426\n",
      "Epoch: 36 ; training loss: 0.6435088515281677\n",
      "Epoch: 37 ; training loss: 0.6431658864021301\n",
      "Epoch: 38 ; training loss: 0.6428274512290955\n",
      "Epoch: 39 ; training loss: 0.6424911022186279\n",
      "Epoch: 40 ; training loss: 0.6421530246734619\n",
      "Epoch: 41 ; training loss: 0.641819953918457\n",
      "Epoch: 42 ; training loss: 0.6414886116981506\n",
      "Epoch: 43 ; training loss: 0.6411568522453308\n",
      "Epoch: 44 ; training loss: 0.6408258676528931\n",
      "Epoch: 45 ; training loss: 0.640495240688324\n",
      "Epoch: 46 ; training loss: 0.6401695609092712\n",
      "Epoch: 47 ; training loss: 0.6398438811302185\n",
      "Epoch: 48 ; training loss: 0.6395127773284912\n",
      "Epoch: 49 ; training loss: 0.6391797065734863\n",
      "Epoch: 50 ; training loss: 0.6388453245162964\n",
      "Epoch: 51 ; training loss: 0.6385090351104736\n",
      "Epoch: 52 ; training loss: 0.6381697654724121\n",
      "Epoch: 53 ; training loss: 0.637829601764679\n",
      "Epoch: 54 ; training loss: 0.6374881267547607\n",
      "Epoch: 55 ; training loss: 0.6371426582336426\n",
      "Epoch: 56 ; training loss: 0.6367945671081543\n",
      "Epoch: 57 ; training loss: 0.6364445686340332\n",
      "Epoch: 58 ; training loss: 0.6360927820205688\n",
      "Epoch: 59 ; training loss: 0.6357375979423523\n",
      "Epoch: 60 ; training loss: 0.6353815793991089\n",
      "Epoch: 61 ; training loss: 0.6350218653678894\n",
      "Epoch: 62 ; training loss: 0.6346558332443237\n",
      "Epoch: 63 ; training loss: 0.6342845559120178\n",
      "Epoch: 64 ; training loss: 0.6339111328125\n",
      "Epoch: 65 ; training loss: 0.6335343718528748\n",
      "Epoch: 66 ; training loss: 0.6331537961959839\n",
      "Epoch: 67 ; training loss: 0.6327694058418274\n",
      "Epoch: 68 ; training loss: 0.6323813796043396\n",
      "Epoch: 69 ; training loss: 0.6319915652275085\n",
      "Epoch: 70 ; training loss: 0.6315990090370178\n",
      "Epoch: 71 ; training loss: 0.6312037110328674\n",
      "Epoch: 72 ; training loss: 0.6308035254478455\n",
      "Epoch: 73 ; training loss: 0.630402684211731\n",
      "Epoch: 74 ; training loss: 0.6300002932548523\n",
      "Epoch: 75 ; training loss: 0.6295949220657349\n",
      "Epoch: 76 ; training loss: 0.6291871666908264\n",
      "Epoch: 77 ; training loss: 0.6287774443626404\n",
      "Epoch: 78 ; training loss: 0.6283655762672424\n",
      "Epoch: 79 ; training loss: 0.6279519200325012\n",
      "Epoch: 80 ; training loss: 0.6275359988212585\n",
      "Epoch: 81 ; training loss: 0.6271176934242249\n",
      "Epoch: 82 ; training loss: 0.6266972422599792\n",
      "Epoch: 83 ; training loss: 0.6262748837471008\n",
      "Epoch: 84 ; training loss: 0.6258500814437866\n",
      "Epoch: 85 ; training loss: 0.6254217624664307\n",
      "Epoch: 86 ; training loss: 0.6249917149543762\n",
      "Epoch: 87 ; training loss: 0.6245588064193726\n",
      "Epoch: 88 ; training loss: 0.6241244077682495\n",
      "Epoch: 89 ; training loss: 0.6236894130706787\n",
      "Epoch: 90 ; training loss: 0.6232529282569885\n",
      "Epoch: 91 ; training loss: 0.622815728187561\n",
      "Epoch: 92 ; training loss: 0.6223782300949097\n",
      "Epoch: 93 ; training loss: 0.6219406127929688\n",
      "Epoch: 94 ; training loss: 0.6215025186538696\n",
      "Epoch: 95 ; training loss: 0.621063768863678\n",
      "Epoch: 96 ; training loss: 0.6206246614456177\n",
      "Epoch: 97 ; training loss: 0.6201855540275574\n",
      "Epoch: 98 ; training loss: 0.6197470426559448\n",
      "Epoch: 99 ; training loss: 0.6193087100982666\n",
      "Epoch: 100 ; training loss: 0.6188705563545227\n",
      "Epoch: 101 ; training loss: 0.6184327602386475\n",
      "Epoch: 102 ; training loss: 0.6179956197738647\n",
      "Epoch: 103 ; training loss: 0.6175592541694641\n",
      "Epoch: 104 ; training loss: 0.6171241998672485\n",
      "Epoch: 105 ; training loss: 0.6166900992393494\n",
      "Epoch: 106 ; training loss: 0.6162570714950562\n",
      "Epoch: 107 ; training loss: 0.6158250570297241\n",
      "Epoch: 108 ; training loss: 0.615394651889801\n",
      "Epoch: 109 ; training loss: 0.6149656176567078\n",
      "Epoch: 110 ; training loss: 0.6145382523536682\n",
      "Epoch: 111 ; training loss: 0.6141125559806824\n",
      "Epoch: 112 ; training loss: 0.6136885285377502\n",
      "Epoch: 113 ; training loss: 0.6132665276527405\n",
      "Epoch: 114 ; training loss: 0.6128464341163635\n",
      "Epoch: 115 ; training loss: 0.612428605556488\n",
      "Epoch: 116 ; training loss: 0.6120129227638245\n",
      "Epoch: 117 ; training loss: 0.6115995049476624\n",
      "Epoch: 118 ; training loss: 0.611188530921936\n",
      "Epoch: 119 ; training loss: 0.6107800006866455\n",
      "Epoch: 120 ; training loss: 0.6103739142417908\n",
      "Epoch: 121 ; training loss: 0.6099705696105957\n",
      "Epoch: 122 ; training loss: 0.6095697283744812\n",
      "Epoch: 123 ; training loss: 0.6091716885566711\n",
      "Epoch: 124 ; training loss: 0.6087765097618103\n",
      "Epoch: 125 ; training loss: 0.6083841919898987\n",
      "Epoch: 126 ; training loss: 0.6079947352409363\n",
      "Epoch: 127 ; training loss: 0.6076081395149231\n",
      "Epoch: 128 ; training loss: 0.6072246432304382\n",
      "Epoch: 129 ; training loss: 0.6068440079689026\n",
      "Epoch: 130 ; training loss: 0.6064667105674744\n",
      "Epoch: 131 ; training loss: 0.6060924530029297\n",
      "Epoch: 132 ; training loss: 0.6057214736938477\n",
      "Epoch: 133 ; training loss: 0.6053534150123596\n",
      "Epoch: 134 ; training loss: 0.6049885153770447\n",
      "Epoch: 135 ; training loss: 0.6046267747879028\n",
      "Epoch: 136 ; training loss: 0.6042680740356445\n",
      "Epoch: 137 ; training loss: 0.6039125919342041\n",
      "Epoch: 138 ; training loss: 0.6035601496696472\n",
      "Epoch: 139 ; training loss: 0.6032109260559082\n",
      "Epoch: 140 ; training loss: 0.6028648018836975\n",
      "Epoch: 141 ; training loss: 0.6025218367576599\n",
      "Epoch: 142 ; training loss: 0.6021819710731506\n",
      "Epoch: 143 ; training loss: 0.6018451452255249\n",
      "Epoch: 144 ; training loss: 0.6015114188194275\n",
      "Epoch: 145 ; training loss: 0.6011806130409241\n",
      "Epoch: 146 ; training loss: 0.6008528470993042\n",
      "Epoch: 147 ; training loss: 0.6005281209945679\n",
      "Epoch: 148 ; training loss: 0.6002063155174255\n",
      "Epoch: 149 ; training loss: 0.5998873710632324\n",
      "Epoch: 150 ; training loss: 0.5995712280273438\n",
      "Epoch: 151 ; training loss: 0.5992579460144043\n",
      "Epoch: 152 ; training loss: 0.5989473462104797\n",
      "Epoch: 153 ; training loss: 0.5986394882202148\n",
      "Epoch: 154 ; training loss: 0.5983342528343201\n",
      "Epoch: 155 ; training loss: 0.5980318188667297\n",
      "Epoch: 156 ; training loss: 0.59773188829422\n",
      "Epoch: 157 ; training loss: 0.5974344611167908\n",
      "Epoch: 158 ; training loss: 0.5971400737762451\n",
      "Epoch: 159 ; training loss: 0.5968480110168457\n",
      "Epoch: 160 ; training loss: 0.5965585112571716\n",
      "Epoch: 161 ; training loss: 0.5962717533111572\n",
      "Epoch: 162 ; training loss: 0.595988392829895\n",
      "Epoch: 163 ; training loss: 0.5957072973251343\n",
      "Epoch: 164 ; training loss: 0.5954285264015198\n",
      "Epoch: 165 ; training loss: 0.5951521992683411\n",
      "Epoch: 166 ; training loss: 0.5948780179023743\n",
      "Epoch: 167 ; training loss: 0.5946059226989746\n",
      "Epoch: 168 ; training loss: 0.5943353772163391\n",
      "Epoch: 169 ; training loss: 0.594066858291626\n",
      "Epoch: 170 ; training loss: 0.5938001871109009\n",
      "Epoch: 171 ; training loss: 0.593535840511322\n",
      "Epoch: 172 ; training loss: 0.5932742953300476\n",
      "Epoch: 173 ; training loss: 0.5930150747299194\n",
      "Epoch: 174 ; training loss: 0.5927579998970032\n",
      "Epoch: 175 ; training loss: 0.5925025343894958\n",
      "Epoch: 176 ; training loss: 0.5922482013702393\n",
      "Epoch: 177 ; training loss: 0.5919954180717468\n",
      "Epoch: 178 ; training loss: 0.5917444229125977\n",
      "Epoch: 179 ; training loss: 0.5914949178695679\n",
      "Epoch: 180 ; training loss: 0.5912469625473022\n",
      "Epoch: 181 ; training loss: 0.591000497341156\n",
      "Epoch: 182 ; training loss: 0.5907557606697083\n",
      "Epoch: 183 ; training loss: 0.5905122756958008\n",
      "Epoch: 184 ; training loss: 0.5902702808380127\n",
      "Epoch: 185 ; training loss: 0.5900301933288574\n",
      "Epoch: 186 ; training loss: 0.5897911787033081\n",
      "Epoch: 187 ; training loss: 0.5895544290542603\n",
      "Epoch: 188 ; training loss: 0.5893197655677795\n",
      "Epoch: 189 ; training loss: 0.5890860557556152\n",
      "Epoch: 190 ; training loss: 0.5888537764549255\n",
      "Epoch: 191 ; training loss: 0.5886223316192627\n",
      "Epoch: 192 ; training loss: 0.5883921980857849\n",
      "Epoch: 193 ; training loss: 0.5881632566452026\n",
      "Epoch: 194 ; training loss: 0.5879353880882263\n",
      "Epoch: 195 ; training loss: 0.5877085328102112\n",
      "Epoch: 196 ; training loss: 0.5874826312065125\n",
      "Epoch: 197 ; training loss: 0.5872575640678406\n",
      "Epoch: 198 ; training loss: 0.5870334506034851\n",
      "Epoch: 199 ; training loss: 0.5868098735809326\n",
      "training finished\n",
      "Accuracy: 0.741935\n"
     ]
    }
   ],
   "source": [
    "model3 = build_model_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
